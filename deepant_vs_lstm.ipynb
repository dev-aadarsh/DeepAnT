{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt                        \n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 1\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN params\n",
    "CNN_w = 45\n",
    "CNN_pred_window = 1\n",
    "CNN_filter1_size = 128\n",
    "CNN_filter2_size = 32\n",
    "CNN_kernel_size = 2\n",
    "CNN_stride = 1\n",
    "CNN_pool_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM params\n",
    "LSTM_w = 45\n",
    "LSTM_pred_window = LSTM_w\n",
    "LSTM_n_layers = 2\n",
    "LSTM_hidden_dim = 256\n",
    "LSTM_kernel_size = 2\n",
    "LSTM_stride = 1\n",
    "LSTM_pool_size = 2\n",
    "LSTM_output_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define CNN architechture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CNN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        ## layers of a CNN\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(1, CNN_filter1_size, CNN_kernel_size, CNN_stride, padding=0)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(CNN_filter1_size, CNN_filter2_size, CNN_kernel_size, CNN_stride, padding=0)\n",
    "\n",
    "        self.maxpool = nn.MaxPool1d(CNN_pool_size)\n",
    "        \n",
    "        self.dim1 = int(0.5 * (0.5 * (CNN_w - 1) - 1)) * CNN_filter2_size\n",
    "        \n",
    "        self.lin1 = nn.Linear(self.dim1, CNN_pred_window)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #convolution layer 1\n",
    "        x = (F.relu(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        #convolution layer 2\n",
    "        x = (F.relu(self.conv2(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = x.view(-1,self.dim1)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.lin1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define LSTM architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self,batch_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=LSTM_w, hidden_size=LSTM_hidden_dim, num_layers=LSTM_n_layers, dropout=0.5)\n",
    "        self.fc = nn.Linear(LSTM_hidden_dim, LSTM_pred_window)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.shape[1]\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = x.contiguous().view(-1, LSTM_hidden_dim)\n",
    "        x = self.dropout(x)\n",
    "        out =  (self.fc(x))\n",
    "        out = out.view(batch_size, -1, LSTM_pred_window)\n",
    "        out = out[:,-1]\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self,size): \n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        hidden = (weight.new(LSTM_n_layers, size, LSTM_hidden_dim).zero_(),\n",
    "                      weight.new(LSTM_n_layers, size, LSTM_hidden_dim).zero_())\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CNN_subsequences(data):\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for i in range(len(data) - CNN_w - CNN_pred_window):\n",
    "        X.append(data[i : i + CNN_w])\n",
    "        Y.append(data[i + CNN_w : i + CNN_w + CNN_pred_window])\n",
    "    \n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LSTM_subsequences(data, ts_data, train_percent, valid_percent):\n",
    "    X = []\n",
    "    Y = []\n",
    "    idx = []\n",
    "    \n",
    "    for i in range(len(data) - LSTM_w):\n",
    "        mean = np.mean(data[i : i + LSTM_w])\n",
    "        std = np.std(data[i : i + LSTM_w])\n",
    "        X.append((data[i : i + LSTM_w]))\n",
    "        Y.append((data[i + 1 : i + LSTM_w + 1]))\n",
    "        idx.append(ts_data.index[train_percent + valid_percent + i + LSTM_w])\n",
    "    \n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    return X, Y,idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deepant(n_epochs, trainX, trainY, validX, validY, model, optimizer, criterion, save_path, freq=20):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "\n",
    "    target_train = torch.tensor(trainY).type('torch.FloatTensor')\n",
    "    data_train = torch.tensor(trainX).type('torch.FloatTensor')\n",
    "    \n",
    "    target_valid = torch.tensor(validY).type('torch.FloatTensor')\n",
    "    data_valid = torch.tensor(validX).type('torch.FloatTensor')\n",
    "    \n",
    "    train_loss_min = np.Inf\n",
    "    valid_loss_min = np.Inf\n",
    "    last_valid_loss= 0\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        \n",
    "        ############\n",
    "        # training #\n",
    "        ############\n",
    "        model.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data_train)\n",
    "        loss = criterion(output, target_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss = loss.item()\n",
    "        \n",
    "        ##############\n",
    "        # validation #\n",
    "        ##############\n",
    "        model.eval()\n",
    "        output_valid = model(data_valid)\n",
    "        \n",
    "        loss_valid = criterion(output_valid, target_valid)\n",
    "        valid_loss = loss_valid.item()\n",
    "        \n",
    "        if(valid_loss == last_valid_loss):\n",
    "            print('problem')\n",
    "            \n",
    "        last_valid_loss = valid_loss\n",
    "        if(epoch%freq == 0):\n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "                epoch, \n",
    "                train_loss,\n",
    "                valid_loss\n",
    "                ), end='\\r')\n",
    "            \n",
    "        # save model if validation loss decreases\n",
    "        if valid_loss < valid_loss_min:\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "\n",
    "    return model, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_back_prop(lstm, optimizer, criterion, inp, target, hidden):\n",
    "    hidden = tuple([each.data for each in hidden])\n",
    "    lstm.zero_grad()\n",
    "    output, hidden = lstm(inp, hidden)\n",
    "    loss = criterion(output.squeeze(), target)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(lstm.parameters(), 5)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), hidden\n",
    "\n",
    "def train_lstm(n_epochs, trainX,trainY, validX,validY,lstm, optimizer, batch_size, \n",
    "               size_valid, size_test, criterion, save_path, freq=20):  \n",
    "    target_train = torch.tensor(trainY).type('torch.FloatTensor')\n",
    "    data_train = torch.tensor(trainX).type('torch.FloatTensor')\n",
    "    \n",
    "    target_valid = torch.tensor(validY).type('torch.FloatTensor')\n",
    "    data_valid = torch.tensor(validX).type('torch.FloatTensor')\n",
    "    \n",
    "    train_loss_min = np.Inf\n",
    "    valid_loss_min = np.Inf\n",
    "    last_valid_loss = 0\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # Training\n",
    "        hidden = lstm.init_hidden(batch_size)\n",
    "\n",
    "        lstm.train()\n",
    "        train_loss,hidden = forward_back_prop(lstm,optimizer,criterion,data_train,target_train,hidden)\n",
    "        \n",
    "        # Validation\n",
    "        lstm.eval()\n",
    "        hidden_valid = lstm.init_hidden(size_valid)\n",
    "        output_valid,hidden_valid = lstm(data_valid,hidden_valid)\n",
    "        \n",
    "        loss_valid = criterion(output_valid.squeeze(), target_valid)\n",
    "        valid_loss = loss_valid.item()\n",
    "        if(valid_loss == last_valid_loss):\n",
    "            print('problem')\n",
    "            \n",
    "        last_valid_loss = valid_loss\n",
    "        if(epoch%freq == 0):\n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "                epoch, \n",
    "                train_loss,\n",
    "                valid_loss\n",
    "                ), end='\\r')\n",
    "        \n",
    "        # save model if validation loss decreases\n",
    "        if valid_loss < valid_loss_min:\n",
    "            torch.save(lstm.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "        \n",
    "    return lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_f_score(ts_data, df_out, thresh):\n",
    "    positives = ts_data.loc[df_out.index].loc[ts_data.is_anomaly == 1].index\n",
    "    negatives = ts_data.loc[df_out.index].loc[ts_data.is_anomaly == 0].index\n",
    "\n",
    "    tp = []\n",
    "    fn = []\n",
    "    fp = []\n",
    "    tn = []\n",
    "    for p in positives:\n",
    "        if p in thresh.index:\n",
    "            tp.append(p)\n",
    "        else:\n",
    "            fn.append(p)\n",
    "\n",
    "    for n in negatives:\n",
    "        if n in thresh.index:\n",
    "            fp.append(n)\n",
    "        else:\n",
    "            tn.append(n)\n",
    "            \n",
    "    recall = len(tp) / (len(tp) + len(fn))\n",
    "    \n",
    "    if recall != 0:\n",
    "        precision = len(tp) / (len(tp) + len(fp))\n",
    "        F_score = 2 * recall * precision / (recall + precision)\n",
    "    else:\n",
    "        F_score = 0\n",
    "    \n",
    "    return F_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use CNN to fit model, predict anomalies and calc score\n",
    "def calc_deepant_performance(filename):\n",
    "    # load dataset from file\n",
    "    ts_data = pd.read_csv(filename, index_col = 0)\n",
    "\n",
    "    # separate test and train\n",
    "    train_percent = int(0.3 * len(ts_data))\n",
    "    valid_percent = int(0.1 * len(ts_data))\n",
    "    test_percent = int(0.6 * len(ts_data))\n",
    "\n",
    "    train_data = list(ts_data.iloc[:train_percent,0])\n",
    "    valid_data = list(ts_data.iloc[train_percent:train_percent + valid_percent,0])\n",
    "    test_data = list(ts_data.iloc[train_percent + valid_percent:,0])\n",
    "\n",
    "    trainX, trainY = get_CNN_subsequences(train_data)\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "    \n",
    "    validX, validY = get_CNN_subsequences(valid_data)    \n",
    "    validX = np.reshape(validX, (validX.shape[0], 1, validX.shape[1]))\n",
    "    \n",
    "    testX, testY = get_CNN_subsequences(test_data)\n",
    "    testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "    # specify and fit model\n",
    "    model = Net()\n",
    "\n",
    "    criterion_scratch = nn.L1Loss()\n",
    "    optimizer_scratch = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-6)\n",
    "\n",
    "    # train model\n",
    "    model, out = train_deepant(NUM_EPOCHS, trainX, trainY, validX, validY, model, optimizer_scratch, \n",
    "                             criterion_scratch, 'model.pt', freq=10)\n",
    "\n",
    "    # load best saved model\n",
    "    model.load_state_dict(torch.load('model.pt'));\n",
    "\n",
    "    # predict value\n",
    "    test_tensor =  torch.tensor(testX).type('torch.FloatTensor')\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    out = model(test_tensor)\n",
    "    out = out.detach().numpy()\n",
    "\n",
    "    df_out = pd.DataFrame()\n",
    "    df_out['pred'] = out[:, 0]\n",
    "    df_out['actual'] = testY[:, 0]\n",
    "\n",
    "    # predict anomalies\n",
    "    df_out['error'] = np.abs(df_out['pred'] - df_out['actual'])\n",
    "    df_out['error_n'] = (df_out['error'] - df_out['error'].mean()) / df_out['error'].std()\n",
    "    df_out.index = ts_data.index[train_percent + valid_percent + CNN_w + CNN_pred_window - 1 : -1]\n",
    "\n",
    "    thresh = df_out.loc[df_out['error_n'].abs() > THRESHOLD]\n",
    "\n",
    "    # calc performance score\n",
    "    f_score = calc_f_score(ts_data, df_out, thresh)\n",
    "    \n",
    "    return f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use LSTM model to fit model, predict anomalies and calc score\n",
    "def calc_lstm_performance(filename):\n",
    "    ts_data = pd.read_csv(filename, index_col = 0)\n",
    "\n",
    "    a = list(ts_data.columns)\n",
    "    a[1] = 'is_anomaly'\n",
    "    ts_data.columns = a\n",
    "\n",
    "    train_percent = int(0.3*len(ts_data))\n",
    "    valid_percent = int(0.1*len(ts_data))\n",
    "    test_percent = int(0.6*len(ts_data))\n",
    "\n",
    "    train_data = list(ts_data.iloc[:train_percent,0])\n",
    "    valid_data = list(ts_data.iloc[train_percent:train_percent+valid_percent,0])\n",
    "    test_data = list(ts_data.iloc[train_percent+valid_percent:,0])\n",
    "\n",
    "    trainX,trainY,_ = get_LSTM_subsequences(train_data, ts_data, train_percent, valid_percent)\n",
    "    trainX = np.reshape(trainX, (1, trainX.shape[0], trainX.shape[1]))\n",
    "\n",
    "    validX,validY,_ = get_LSTM_subsequences(valid_data, ts_data, train_percent, valid_percent)\n",
    "    validX = np.reshape(validX, (1, validX.shape[0], validX.shape[1]))\n",
    "\n",
    "    testX,testY,test_idx = get_LSTM_subsequences(test_data, ts_data, train_percent, valid_percent)\n",
    "    testX = np.reshape(testX,(1,testX.shape[0],testX.shape[1]))\n",
    "\n",
    "    batch_size = trainX.shape[1]\n",
    "\n",
    "    size_valid = validX.shape[1]\n",
    "    size_test = testX.shape[1]\n",
    "\n",
    "    lstm = LSTM(batch_size)\n",
    "\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(lstm.parameters(), lr=LEARNING_RATE, weight_decay=1e-6)\n",
    "\n",
    "    lstm = train_lstm(NUM_EPOCHS, trainX,trainY, validX,validY, lstm, optimizer, batch_size,\n",
    "                      size_valid, size_test, criterion, 'lstm.pt', freq=10)\n",
    "\n",
    "    lstm.load_state_dict(torch.load('lstm.pt'))\n",
    "\n",
    "    test_tensor =  torch.tensor(testX).type('torch.FloatTensor')\n",
    "    lstm.eval()\n",
    "\n",
    "    hidden = lstm.init_hidden(size_test)\n",
    "    out,hidden = lstm(test_tensor,hidden)\n",
    "    out = out.detach().numpy()\n",
    "\n",
    "    df_out = pd.DataFrame()\n",
    "    df_out['pred'] = out[:,-1]\n",
    "    df_out['actual'] = testY[:,-1]\n",
    "\n",
    "    df_out['error'] =np.abs(df_out['pred'] - df_out['actual'])\n",
    "    df_out['error_n'] = (df_out['error'] - df_out['error'].mean())/df_out['error'].std()\n",
    "\n",
    "    df_out.index = test_idx\n",
    "\n",
    "    thresh = df_out.loc[df_out['error_n'].abs() > THRESHOLD]\n",
    "    thresh['is_anomaly'] = ts_data.loc[thresh.index,'is_anomaly']\n",
    "\n",
    "    # calc performance score\n",
    "    f_score = calc_f_score(ts_data, df_out, thresh)\n",
    "    \n",
    "    return f_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run for one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "yahoo_folder = 'ydata-labeled-time-series-anomalies-v1_0'\n",
    "synthetic_folder = 'synthetic-labeled-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_in_folder(folder_name):\n",
    "    ret_val = os.listdir(folder_name)\n",
    "    ret_val = [folder_name + '/' + x for x in ret_val if 'all' not in x]\n",
    "    ret_val = [x for x in ret_val if 'csv' in x]\n",
    "    \n",
    "    return ret_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ydata-labeled-time-series-anomalies-v1_0/A4Benchmark/A4Benchmark-TS90.csv',\n",
       " 'ydata-labeled-time-series-anomalies-v1_0/A4Benchmark/A4Benchmark-TS91.csv',\n",
       " 'ydata-labeled-time-series-anomalies-v1_0/A4Benchmark/A4Benchmark-TS92.csv',\n",
       " 'ydata-labeled-time-series-anomalies-v1_0/A4Benchmark/A4Benchmark-TS93.csv',\n",
       " 'ydata-labeled-time-series-anomalies-v1_0/A4Benchmark/A4Benchmark-TS94.csv',\n",
       " 'ydata-labeled-time-series-anomalies-v1_0/A4Benchmark/A4Benchmark-TS95.csv',\n",
       " 'ydata-labeled-time-series-anomalies-v1_0/A4Benchmark/A4Benchmark-TS96.csv',\n",
       " 'ydata-labeled-time-series-anomalies-v1_0/A4Benchmark/A4Benchmark-TS97.csv',\n",
       " 'ydata-labeled-time-series-anomalies-v1_0/A4Benchmark/A4Benchmark-TS98.csv',\n",
       " 'ydata-labeled-time-series-anomalies-v1_0/A4Benchmark/A4Benchmark-TS99.csv']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list = []\n",
    "file_list += get_files_in_folder(yahoo_folder + '/A1Benchmark')\n",
    "file_list += get_files_in_folder(yahoo_folder + '/A2Benchmark')\n",
    "file_list += get_files_in_folder(yahoo_folder + '/A3Benchmark')\n",
    "file_list += get_files_in_folder(yahoo_folder + '/A4Benchmark')\n",
    "file_list += get_files_in_folder(synthetic_folder)\n",
    "\n",
    "file_list.sort()\n",
    "\n",
    "file_list[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synthetic-labeled-data/ma.csv\n",
      "Training for 50 epoch(s)...2.616051 \tValidation Loss: 35.249767\n",
      "synthetic-labeled-data/mmm.csv71584 \tValidation Loss: 57.579155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aadarsh/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50 epoch(s)...6.502653 \tValidation Loss: 19.113787\n",
      "synthetic-labeled-data/msft.csv2769 \tValidation Loss: 78.248215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aadarsh/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50 epoch(s)....535954 \tValidation Loss: 1.87606875\n",
      "synthetic-labeled-data/nke.csv66077 \tValidation Loss: 25.514366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aadarsh/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50 epoch(s)...0.513727 \tValidation Loss: 12.106659\n",
      "synthetic-labeled-data/t.csv.053833 \tValidation Loss: 13.843182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aadarsh/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50 epoch(s)....384853 \tValidation Loss: 1.20678209\n",
      "Epoch: 50 \tTraining Loss: 39.404255 \tValidation Loss: 41.065113\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aadarsh/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "output_df = pd.DataFrame(columns=['filename', 'deepant_fscore', 'lstm_fscore'])\n",
    "\n",
    "for file in file_list[5:10]:\n",
    "    print(file)\n",
    "    deepant_fscore = calc_deepant_performance(file)\n",
    "    lstm_fscore = calc_lstm_performance(file)\n",
    "    \n",
    "    output_df = output_df.append({'filename': file,\n",
    "                                  'deepant_fscore': deepant_fscore,\n",
    "                                  'lstm_fscore': lstm_fscore}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>deepant_fscore</th>\n",
       "      <th>lstm_fscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>synthetic-labeled-data/ma.csv</td>\n",
       "      <td>0.035191</td>\n",
       "      <td>0.033613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>synthetic-labeled-data/mmm.csv</td>\n",
       "      <td>0.062893</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>synthetic-labeled-data/msft.csv</td>\n",
       "      <td>0.040678</td>\n",
       "      <td>0.025806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>synthetic-labeled-data/nke.csv</td>\n",
       "      <td>0.101010</td>\n",
       "      <td>0.093458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>synthetic-labeled-data/t.csv</td>\n",
       "      <td>0.063492</td>\n",
       "      <td>0.012346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          filename  deepant_fscore  lstm_fscore\n",
       "0    synthetic-labeled-data/ma.csv        0.035191     0.033613\n",
       "1   synthetic-labeled-data/mmm.csv        0.062893     0.037037\n",
       "2  synthetic-labeled-data/msft.csv        0.040678     0.025806\n",
       "3   synthetic-labeled-data/nke.csv        0.101010     0.093458\n",
       "4     synthetic-labeled-data/t.csv        0.063492     0.012346"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
