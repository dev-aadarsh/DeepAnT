{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt                        \n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 1\n",
    "NUM_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN params\n",
    "w = 45\n",
    "pred_window = 1\n",
    "filter1_size = 128\n",
    "filter2_size = 32\n",
    "kernel_size = 2\n",
    "stride = 1\n",
    "pool_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define CNN architechture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CNN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        ## layers of a CNN\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(1,filter1_size,kernel_size,stride,padding = 0)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(filter1_size,filter2_size,kernel_size,stride,padding = 0)\n",
    "\n",
    "        self.maxpool = nn.MaxPool1d(pool_size)\n",
    "        \n",
    "        self.dim1 = int(0.5*(0.5*(w-1)-1)) * filter2_size\n",
    "        \n",
    "        self.lin1 = nn.Linear(self.dim1,pred_window )\n",
    "\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #convolution layer 1\n",
    "        x = (F.relu(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        #convolution layer 2\n",
    "        x = (F.relu(self.conv2(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = x.view(-1,self.dim1)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.lin1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subsequences(data):\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for i in range(len(data) - w - pred_window):\n",
    "        X.append(data[i : i + w])\n",
    "        Y.append(data[i + w : i + w + pred_window])\n",
    "    \n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    X = np.reshape(X, (X.shape[0], 1, X.shape[1]))\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid(n_epochs, trainX, trainY, validX, validY, model, optimizer, criterion, save_path, freq=20):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "\n",
    "    target_train = torch.tensor(trainY).type('torch.FloatTensor')\n",
    "    data_train = torch.tensor(trainX).type('torch.FloatTensor')\n",
    "    \n",
    "    target_valid = torch.tensor(validY).type('torch.FloatTensor')\n",
    "    data_valid = torch.tensor(validX).type('torch.FloatTensor')\n",
    "    \n",
    "    train_loss_min = np.Inf\n",
    "    valid_loss_min = np.Inf\n",
    "    last_valid_loss= 0\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        \n",
    "        ############\n",
    "        # training #\n",
    "        ############\n",
    "        model.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data_train)\n",
    "        loss = criterion(output, target_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss = loss.item()\n",
    "        \n",
    "        ##############\n",
    "        # validation #\n",
    "        ##############\n",
    "        model.eval()\n",
    "        output_valid = model(data_valid)\n",
    "        \n",
    "        loss_valid = criterion(output_valid, target_valid)\n",
    "        valid_loss = loss_valid.item()\n",
    "        \n",
    "        if(valid_loss == last_valid_loss):\n",
    "            print('problem')\n",
    "            \n",
    "        last_valid_loss = valid_loss\n",
    "        if(epoch%freq == 0):\n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "                epoch, \n",
    "                train_loss,\n",
    "                valid_loss\n",
    "                ), end='\\r')\n",
    "            \n",
    "        # save model if validation loss decreases\n",
    "        if valid_loss < valid_loss_min:\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "\n",
    "    return model,output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_f_score(ts_data, df_out, thresh):\n",
    "    positives = ts_data.loc[df_out.index].loc[ts_data.is_anomaly == 1].index\n",
    "    negatives = ts_data.loc[df_out.index].loc[ts_data.is_anomaly == 0].index\n",
    "\n",
    "    tp = []\n",
    "    fn = []\n",
    "    fp = []\n",
    "    tn = []\n",
    "    for p in positives:\n",
    "        if p in thresh.index:\n",
    "            tp.append(p)\n",
    "        else:\n",
    "            fn.append(p)\n",
    "\n",
    "    for n in negatives:\n",
    "        if n in thresh.index:\n",
    "            fp.append(n)\n",
    "        else:\n",
    "            tn.append(n)\n",
    "            \n",
    "    recall = len(tp) / (len(tp) + len(fn))\n",
    "    \n",
    "    if recall != 0:\n",
    "        precision = len(tp) / (len(tp) + len(fp))\n",
    "        F_score = 2 * recall * precision / (recall + precision)\n",
    "    else:\n",
    "        F_score = 0\n",
    "    \n",
    "    return F_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function to fit model, predict anomalies and calc score\n",
    "def calc_model_performance(filename):\n",
    "    # load dataset from file\n",
    "    ts_data = pd.read_csv(filename, index_col = 0)\n",
    "\n",
    "    # separate test and train\n",
    "    train_percent = int(0.3 * len(ts_data))\n",
    "    valid_percent = int(0.1 * len(ts_data))\n",
    "    test_percent = int(0.6 * len(ts_data))\n",
    "\n",
    "    train_data = list(ts_data.iloc[:train_percent,0])\n",
    "    valid_data = list(ts_data.iloc[train_percent:train_percent + valid_percent,0])\n",
    "    test_data = list(ts_data.iloc[train_percent + valid_percent:,0])\n",
    "\n",
    "    trainX, trainY = get_subsequences(train_data)\n",
    "    validX, validY = get_subsequences(valid_data)\n",
    "    testX, testY = get_subsequences(test_data)\n",
    "\n",
    "    # specify and fit model\n",
    "    model = Net()\n",
    "\n",
    "    criterion_scratch = nn.L1Loss()\n",
    "    optimizer_scratch = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-6)\n",
    "\n",
    "    # train model\n",
    "    model, out = train_valid(NUM_EPOCHS, trainX, trainY, validX, validY, model, optimizer_scratch, \n",
    "                             criterion_scratch, 'model.pt', freq = 10)\n",
    "\n",
    "    # load best saved model\n",
    "    model.load_state_dict(torch.load('model.pt'));\n",
    "\n",
    "    # predict value\n",
    "    test_tensor =  torch.tensor(testX).type('torch.FloatTensor')\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    out = model(test_tensor)\n",
    "    out = out.detach().numpy()\n",
    "\n",
    "    df_out = pd.DataFrame()\n",
    "    df_out['pred'] = out[:, 0]\n",
    "    df_out['actual'] = testY[:, 0]\n",
    "\n",
    "    # predict anomalies\n",
    "    df_out['error'] = np.abs(df_out['pred'] - df_out['actual'])\n",
    "    df_out['error_n'] = (df_out['error'] - df_out['error'].mean()) / df_out['error'].std()\n",
    "    df_out.index = ts_data.index[train_percent + valid_percent + w + pred_window - 1 : -1]\n",
    "\n",
    "    thresh = df_out.loc[df_out['error_n'].abs() > THRESHOLD]\n",
    "\n",
    "    # calc performance score\n",
    "    f_score = calc_f_score(ts_data, df_out, thresh)\n",
    "    \n",
    "    return f_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run for one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "yahoo_folder = 'ydata-labeled-time-series-anomalies-v1_0'\n",
    "synthetic_folder = 'synthetic-labeled-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_in_folder(folder_name):\n",
    "    ret_val = os.listdir(folder_name)\n",
    "    ret_val = [folder_name + '/' + x for x in ret_val if 'all' not in x]\n",
    "    \n",
    "    return ret_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ydata-labeled-time-series-anomalies-v1_0/A4Benchmark/A4Benchmark-TS90.csv',\n",
       " 'ydata-labeled-time-series-anomalies-v1_0/A4Benchmark/A4Benchmark-TS91.csv',\n",
       " 'ydata-labeled-time-series-anomalies-v1_0/A4Benchmark/A4Benchmark-TS92.csv',\n",
       " 'ydata-labeled-time-series-anomalies-v1_0/A4Benchmark/A4Benchmark-TS93.csv',\n",
       " 'ydata-labeled-time-series-anomalies-v1_0/A4Benchmark/A4Benchmark-TS94.csv',\n",
       " 'ydata-labeled-time-series-anomalies-v1_0/A4Benchmark/A4Benchmark-TS95.csv',\n",
       " 'ydata-labeled-time-series-anomalies-v1_0/A4Benchmark/A4Benchmark-TS96.csv',\n",
       " 'ydata-labeled-time-series-anomalies-v1_0/A4Benchmark/A4Benchmark-TS97.csv',\n",
       " 'ydata-labeled-time-series-anomalies-v1_0/A4Benchmark/A4Benchmark-TS98.csv',\n",
       " 'ydata-labeled-time-series-anomalies-v1_0/A4Benchmark/A4Benchmark-TS99.csv']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list = []\n",
    "file_list += get_files_in_folder(yahoo_folder + '/A1Benchmark')\n",
    "file_list += get_files_in_folder(yahoo_folder + '/A2Benchmark')\n",
    "file_list += get_files_in_folder(yahoo_folder + '/A3Benchmark')\n",
    "file_list += get_files_in_folder(yahoo_folder + '/A4Benchmark')\n",
    "file_list += get_files_in_folder(synthetic_folder)\n",
    "\n",
    "file_list.sort()\n",
    "\n",
    "file_list[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synthetic-labeled-data/^gspc.csv\n",
      "synthetic-labeled-data/aapl.csv07488.000000 \tValidation Loss: 1070351936.000000\n",
      "synthetic-labeled-data/ge.csv9044096.000000 \tValidation Loss: 1294660224.000000\n",
      "synthetic-labeled-data/gs.csv2952064.000000 \tValidation Loss: 1268404736.000000\n",
      "synthetic-labeled-data/ko.csv3390464.000000 \tValidation Loss: 1422883968.000000\n",
      "Epoch: 50 \tTraining Loss: 965408512.000000 \tValidation Loss: 1008013504.0000000\r"
     ]
    }
   ],
   "source": [
    "output_df = pd.DataFrame(columns=['filename', 'deepant_fscore'])\n",
    "\n",
    "for file in file_list[:5]:\n",
    "    print(file)\n",
    "    fscore = calc_model_performance(file)\n",
    "    \n",
    "    output_df = output_df.append({'filename':file, 'deepant_fscore':fscore}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>deepant_fscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>synthetic-labeled-data/^gspc.csv</td>\n",
       "      <td>0.035714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>synthetic-labeled-data/aapl.csv</td>\n",
       "      <td>0.019544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>synthetic-labeled-data/ge.csv</td>\n",
       "      <td>0.033708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>synthetic-labeled-data/gs.csv</td>\n",
       "      <td>0.079051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>synthetic-labeled-data/ko.csv</td>\n",
       "      <td>0.061224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           filename  deepant_fscore\n",
       "0  synthetic-labeled-data/^gspc.csv        0.035714\n",
       "1   synthetic-labeled-data/aapl.csv        0.019544\n",
       "2     synthetic-labeled-data/ge.csv        0.033708\n",
       "3     synthetic-labeled-data/gs.csv        0.079051\n",
       "4     synthetic-labeled-data/ko.csv        0.061224"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
